# Claude on 3rd-Party Platforms - Working Examples

Production-ready code examples for AWS Bedrock and Google Vertex AI.

## ğŸ“ Quick File Reference

```
claude-3rd-party-platforms/
â”œâ”€â”€ README.md                    â† Start here (setup & guide)
â”œâ”€â”€ INDEX.md                     â† This file
â”œâ”€â”€ requirements.txt             â† Python dependencies
â”œâ”€â”€ package.json                 â† TypeScript dependencies
â”‚
â”œâ”€â”€ AWS Bedrock Examples (Python):
â”œâ”€â”€ bedrock_basic.py            â­ Simple message
â”œâ”€â”€ bedrock_streaming.py        â­â­ Real-time responses
â”œâ”€â”€ bedrock_error_handling.py   â­â­â­ Retry logic
â”œâ”€â”€ bedrock_conversation.py     â­â­â­ Multi-turn chat
â”œâ”€â”€ bedrock_vision.py           â­â­â­ Image analysis
â”‚
â”œâ”€â”€ Google Vertex AI Examples (Python):
â”œâ”€â”€ vertex_ai_basic.py          â­ Simple message
â”œâ”€â”€ vertex_ai_streaming.py      â­â­ Real-time responses
â”‚
â””â”€â”€ TypeScript Examples:
    â”œâ”€â”€ bedrock_basic.ts        â­ Bedrock simple
    â””â”€â”€ vertex_ai_basic.ts      â­ Vertex simple
```

## ğŸš€ Quick Start (5 minutes)

### Option 1: AWS Bedrock (Python)

```bash
# 1. Install dependencies
pip install -U "anthropic[bedrock]"

# 2. Configure AWS
aws configure

# 3. Run example
python bedrock_basic.py
```

### Option 2: Google Vertex AI (Python)

```bash
# 1. Install dependencies
pip install -U google-cloud-aiplatform "anthropic[vertex]"

# 2. Authenticate
gcloud auth application-default login

# 3. Edit file (replace YOUR_PROJECT_ID)
nano vertex_ai_basic.py

# 4. Run example
python vertex_ai_basic.py
```

### Option 3: TypeScript

```bash
# 1. Install dependencies
npm install

# 2. Run Bedrock example
npm run bedrock:basic

# 3. Or run Vertex example
npm run vertex:basic
```

## ğŸ“Š Example Comparison

| Name | Type | Complexity | Use Case |
|------|------|-----------|----------|
| `bedrock_basic.py` | Python | â­ | Learn the basics |
| `bedrock_streaming.py` | Python | â­â­ | Long responses |
| `bedrock_error_handling.py` | Python | â­â­â­ | Production code |
| `bedrock_conversation.py` | Python | â­â­â­ | Chatbots |
| `bedrock_vision.py` | Python | â­â­â­ | Image analysis |
| `vertex_ai_basic.py` | Python | â­ | Learn Vertex |
| `vertex_ai_streaming.py` | Python | â­â­ | Streaming on Vertex |
| `bedrock_basic.ts` | TypeScript | â­ | TypeScript/JS users |
| `vertex_ai_basic.ts` | TypeScript | â­ | TypeScript/JS users |

## ğŸ¯ Learning Path

### Beginner (â­)
1. Read: README.md
2. Run: `bedrock_basic.py`
3. Modify: Change the prompt, try different models

### Intermediate (â­â­)
1. Run: `bedrock_streaming.py`
2. Run: `vertex_ai_basic.py`
3. Modify: Add a system prompt to any example

### Advanced (â­â­â­)
1. Study: `bedrock_error_handling.py`
2. Study: `bedrock_conversation.py`
3. Study: `bedrock_vision.py`
4. Combine patterns for your use case

## ğŸ“ Running Each Example

### bedrock_basic.py
```bash
python bedrock_basic.py
# Output: Simple response with token count
```

### bedrock_streaming.py
```bash
python bedrock_streaming.py
# Output: Real-time streaming response
```

### bedrock_error_handling.py
```bash
python bedrock_error_handling.py
# Output: Response with retry logic demo
```

### bedrock_conversation.py
```bash
python bedrock_conversation.py
# Interactive chat session
# Commands: quit, history
```

### bedrock_vision.py
```bash
python bedrock_vision.py /path/to/image.png
# Output: Image analysis
```

### vertex_ai_basic.py
```bash
# Edit file first: replace YOUR_PROJECT_ID
python vertex_ai_basic.py
# Output: Simple response from Vertex AI
```

### vertex_ai_streaming.py
```bash
# Edit file first: replace YOUR_PROJECT_ID
python vertex_ai_streaming.py
# Output: Streaming response from Vertex AI
```

### TypeScript Examples
```bash
npm run bedrock:basic
npm run vertex:basic
```

## ğŸ”§ Common Modifications

### Change Model
**Python:**
```python
model="global.anthropic.claude-haiku-4-5-20251001-v1:0"  # Haiku instead
```

**TypeScript:**
```typescript
model: 'claude-haiku-4-5@20251001'  // Haiku instead
```

### Change Region
**Bedrock:**
```python
client = AnthropicBedrock(aws_region="eu-west-1")
```

**Vertex:**
```python
client = AnthropicVertex(project_id="my-project", region="europe-west1")
```

### Add System Prompt
```python
response = client.messages.create(
    model="...",
    max_tokens=256,
    system="You are a helpful Python expert.",
    messages=[...]
)
```

### Increase Max Tokens
```python
response = client.messages.create(
    model="...",
    max_tokens=1024,  # Changed from 256
    messages=[...]
)
```

## ğŸ“š Understanding the Code

### Basic Pattern
```python
from anthropic import AnthropicBedrock

# 1. Initialize
client = AnthropicBedrock(aws_region="us-west-2")

# 2. Send message
message = client.messages.create(
    model="global.anthropic.claude-sonnet-4-5-20250929-v1:0",
    max_tokens=256,
    messages=[{"role": "user", "content": "Hello!"}]
)

# 3. Get response
text = message.content[0].text
print(text)
```

### Streaming Pattern
```python
with client.messages.stream(...) as stream:
    for text in stream.text_stream:
        print(text, end="", flush=True)
```

### Error Handling Pattern
```python
try:
    response = client.messages.create(...)
except RateLimitError:
    # Handle rate limit
    time.sleep(2)
except APIError as e:
    # Handle other API errors
    print(f"Error: {e}")
```

### Conversation Pattern
```python
conversation = []
# Add user message
conversation.append({"role": "user", "content": "Hello"})
# Get response (context-aware)
response = client.messages.create(..., messages=conversation)
# Add assistant response for next turn
conversation.append({"role": "assistant", "content": response_text})
```

## ğŸ”‘ Key Concepts

**Model IDs:**
- Bedrock: `global.anthropic.claude-*-20250929-v1:0` (global endpoints)
- Vertex: `claude-*@20250929` (regional routing via `region` parameter)

**Token Usage:**
```python
print(message.usage.input_tokens)   # Tokens in your input
print(message.usage.output_tokens)  # Tokens in response
```

**Streaming vs Non-Streaming:**
- Use streaming for long responses (better UX)
- Use non-streaming for short responses (simpler code)

**Max Tokens:**
- Set based on expected response length
- Smaller values = faster/cheaper
- Larger values = more flexibility

## ğŸ› Debugging Tips

**Check credentials:**
```bash
# Bedrock
aws sts get-caller-identity

# Vertex
gcloud auth application-default login
```

**Check region/project:**
```bash
# Bedrock
aws configure get region

# Vertex
gcloud config get-value project
```

**Enable debug logging:**
```python
import logging
logging.basicConfig(level=logging.DEBUG)
```

**Test with Haiku (fast/cheap):**
```python
model="global.anthropic.claude-haiku-4-5-20251001-v1:0"
```

## ğŸ“– Full Documentation

For complete documentation, see:
- `../../docs/claude-3rd-party-platforms/README.md` - Platform overview
- `../../docs/claude-3rd-party-platforms/bedrock-setup-guide.md` - Bedrock setup
- `../../docs/claude-3rd-party-platforms/vertex-ai-setup-guide.md` - Vertex setup
- `../../docs/claude-3rd-party-platforms/code-examples.md` - More patterns

## ğŸ’¡ Next Steps

1. âœ… Pick a platform (Bedrock or Vertex)
2. âœ… Run the basic example
3. âœ… Read the comprehensive documentation
4. âœ… Try streaming and conversation examples
5. âœ… Adapt code for your use case
6. ğŸš€ Deploy to production

## ğŸ“ Getting Help

**Setup issues?**
â†’ Check platform-specific setup guides in `docs/`

**Code issues?**
â†’ Check README.md Troubleshooting section

**Want more examples?**
â†’ See `docs/claude-3rd-party-platforms/code-examples.md`

---

**Happy coding! ğŸš€**
